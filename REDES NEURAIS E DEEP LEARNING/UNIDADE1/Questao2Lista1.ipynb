{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2\n",
        "\n",
        "Considere a função E(w) onde w = $[w_1, w_2, ..., w_n]^t$ é um vetor com múltiplas variáveis.\n",
        "Usando a expansão em série de Taylor a função pode ser expressa como $E(w(n+Δw(n))) = E(w(n)) + g^T(w(n)) ⋅ Δw(n) + (1/2) Δw^T(n) H(w(n)) Δw(n) + O(|Δw|^3)$\n",
        "onde g(w(n)) é o vetor gradiente local definido por g(w) = $∂E(w) / ∂w$ e\n",
        "H(w) é matriz Hessiana, definida por $∂²w / ∂E(w)²$.\n",
        "<br>\n",
        "Demonstre com base na expansão em série de Taylor:"
      ],
      "metadata": {
        "id": "ntOQ01aa8OxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a-) que o método do gradiente da descida mais íngreme é dado por $w(n+1) = w(n) - α ⋅ g(w(n))$"
      ],
      "metadata": {
        "id": "nY0wY-VY9hgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partindo da expansão em série de Taylor fornecida:\n",
        "\n",
        "$E(w(n+\\Delta w)) = E(w(n)) + g^T(w(n)) \\cdot \\Delta w + \\frac{1}{2} \\Delta w^T H(w(n)) \\Delta w + O(|\\Delta w|^3)$\n",
        "\n",
        "Para encontrar o mínimo local de \\( E(w) \\), queremos minimizar $E(w(n+\\Delta w))$ em relação a $(\\Delta w)$. Assim, podemos descartar termos de ordem superior a 1 em $\\Delta w $. Isso nos leva a:\n",
        "\n",
        "$Delta w \\approx g^T(w(n)) \\cdot \\Delta w$\n",
        "\n",
        "Agora, para minimizar $E(w(n+\\Delta w))$, devemos escolher $\\Delta w $ tal que $ \\Delta E $ seja negativo, ou seja, $\\Delta w $ deve ser oposto ao gradiente $ g(w(n))$. Portanto, podemos escolher $ \\Delta w = -\\alpha \\cdot g(w(n)) $, onde $ \\alpha $ é um fator de aprendizado positivo.\n",
        "\n",
        "Então, temos:\n",
        "\n",
        "$ \\Delta E \\approx g^T(w(n)) \\cdot (-\\alpha \\cdot g(w(n))) = -\\alpha ||g(w(n))||^2 $\n",
        "\n",
        "Isso mostra que $ \\Delta E $ é negativo, indicando que estamos descendo na direção do gradiente. Portanto, o método do gradiente da descida mais íngreme é dado por:\n",
        "\n",
        "$w(n + 1) = w(n) - \\alpha \\cdot g(w(n)) $"
      ],
      "metadata": {
        "id": "0ssuJg2z9wQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b-) que o método de Newton é dado por $w(n+1) = w(n) + H^{-1}(w(n)) ⋅ g(w(n))$\n"
      ],
      "metadata": {
        "id": "DnPJyOvS-4cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A expansão em série de Taylor nos dá:\n",
        "\n",
        "$ E(w(n+\\Delta w)) = E(w(n)) + g^T(w(n)) \\cdot \\Delta w + \\frac{1}{2} \\Delta w^T H(w(n)) \\Delta w + O(|\\Delta w|^3) $\n",
        "\n",
        "Para encontrar o mínimo local de $ E(w) $, queremos minimizar $ E(w(n+\\Delta w)) $ em relação a $ \\Delta w $. Tomando a primeira derivada em relação a $ \\Delta w $ e igualando a zero para encontrar o mínimo, temos:\n",
        "\n",
        "$\\frac{\\partial \\Delta w}{\\partial E} = g(w(n)) + H(w(n)) \\cdot \\Delta w = 0 $\n",
        "\n",
        "Isolando $ \\Delta w $, obtemos:\n",
        "\n",
        "$\\Delta w = -H^{-1}(w(n)) \\cdot g(w(n)) $\n",
        "\n",
        "Substituindo $ \\Delta w $ de volta na expressão de $ w(n+\\Delta w) $, obtemos:\n",
        "\n",
        "$ w(n+1) = w(n) + H^{-1}(w(n)) \\cdot g(w(n)) $\n",
        "\n",
        "Portanto, o método de Newton é dado por:\n",
        "\n",
        "$ w(n+1) = w(n) + H^{-1}(w(n)) \\cdot g(w(n)) $\n"
      ],
      "metadata": {
        "id": "fFC4yZ--_N5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c-) Que o passo ótimo é dado por $g(w) * g^T(w) / (g^T(w) * H(w) * g(w))$ assumindo a matriz H definada\n",
        "positiva\n"
      ],
      "metadata": {
        "id": "t3XmzeuFAExB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Começamos com a expansão em série de Taylor novamente:\n",
        "\n",
        "$E(w(n + Δw)) = E(w(n)) + g^T(w(n)) · Δw + 1/2 Δw^T · H(w(n)) · Δw + O(|Δw|^3)$\n",
        "\n",
        "Queremos minimizar $E(w(n + Δw))$ em relação a Δw. Para encontrar o mínimo, podemos diferenciar $E(w(n + Δw))$ em relação a Δw e igualar a zero:\n",
        "\n",
        "$∂E/∂Δw = g(w(n)) + H(w(n)) · Δw = 0$\n",
        "\n",
        "Isolando Δw:\n",
        "\n",
        "$Δw = -H^(-1)(w(n)) · g(w(n))$\n",
        "\n",
        "Substituindo Δw de volta na expressão de $E(w(n + Δw))$, obtemos:\n",
        "\n",
        "$E(w(n + Δw)) = E(w(n)) - g^T(w(n)) · H^(-1)(w(n)) · g(w(n)) + O(|Δw|^3)$\n",
        "\n",
        "Agora, queremos encontrar o valor de Δw que minimiza $E(w(n + Δw))$, então estamos interessados no termo $g^T(w(n)) · H^(-1)(w(n)) · g(w(n))$.\n",
        "\n",
        "Se tomarmos $Δw = -α · g(w)$, onde α é um fator de aprendizado, e substituirmos isso em $E(w(n + Δw))$, obteremos:\n",
        "\n",
        "$E(w(n + Δw)) = E(w(n)) - α · g^T(w(n)) · H^(-1)(w(n)) · g(w(n)) + O(α^3)$\n",
        "\n",
        "Agora, podemos observar que $E(w(n + Δw))$ é minimizado quando α é escolhido de forma que o termo $-α · g^T(w(n)) · H^(-1)(w(n)) · g(w(n))$ seja maximizado.\n",
        "\n",
        "Dado que H é definida positiva, então $H^{-1}$ também é definida positiva. Isso significa que para qualquer vetor não nulo g(w), o termo $g^T(w) · H^(-1)(w) · g(w)$ é sempre positivo. Assim, para maximizar $-α · g^T(w(n)) · H^(-1)(w(n)) · g(w(n))$, devemos minimizar α.\n",
        "\n",
        "A escolha ótima de α que minimiza este termo é α = 1, uma vez que a derivada segunda do erro com relação ao passo (segunda derivada de E em relação a Δw) é dada pela matriz H, que é definida positiva. Portanto, o passo ótimo é dado por α = 1.\n",
        "\n",
        "Portanto, o passo ótimo é dado por:\n",
        "\n",
        "Passo ótimo = $(g(w) · g^T(w)) / (g^T(w) · H(w) · g(w))$\n"
      ],
      "metadata": {
        "id": "HC0xrDnSAjP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d-) Qual será o valor do passo sob as condições acima se vetor gradiente estiver alinhado com a autovetor correspondente ao autovalor máximo."
      ],
      "metadata": {
        "id": "77GFcS18B1HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assumimos que o vetor gradiente g(w) está alinhado com o autovetor correspondente ao autovalor máximo da matriz Hessiana H(w).\n",
        "\n",
        "Do passo ótimo (demonstrado na letra c)), temos:\n",
        "\n",
        "Passo ótimo = $(g(w) · g^T(w)) / (g^T(w) · H(w) · g(w))$\n",
        "\n",
        "Substituindo $g(w) = λ_max · v$, onde $λ_max$ é o autovalor máximo de H(w) e v é o autovetor correspondente:\n",
        "\n",
        "Passo ótimo = $((λ_max · v) · (λ_max · v)^T) / ((λ_max · v)^T · H(w) · (λ_max · v))$\n",
        "\n",
        "Como v é um autovetor unitário, $v^T · v = 1$. Além disso, $H(w) · v = λ_max · v$. Substituindo essas informações, temos:\n",
        "\n",
        "Passo ótimo = $(λ_max^2 · v · v^T) / (λ_max · v^T · λ_max · v)$\n",
        "\n",
        "Cancelando $λ_max$ de cima e de baixo, obtemos:\n",
        "\n",
        "Passo ótimo = $(v · v^T) / (v^T · v)$\n",
        "\n",
        "Como v é um autovetor unitário, $v^T · v = 1$, então:\n",
        "\n",
        "Passo ótimo = $v · v^T$\n",
        "\n",
        "Portanto, sob as condições dadas, o valor do passo ótimo é $v · v^T$, onde v é o autovetor correspondente ao autovalor máximo da matriz Hessiana.\n"
      ],
      "metadata": {
        "id": "LAl3AmLnM5pV"
      }
    }
  ]
}