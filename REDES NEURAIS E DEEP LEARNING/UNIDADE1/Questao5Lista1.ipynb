{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfMIuASbdjxj"
      },
      "source": [
        "# 5.\n",
        "\n",
        "Apresente um estudo dos seguintes algoritmos de otimiza√ß√£o:\n",
        "\n",
        "- **Gradiente Estoc√°stico (SGD)**\n",
        "- **AdaGrad**\n",
        "- **RMSProp**\n",
        "- **Adam**\n",
        "\n",
        "Estes m√©todos ou otimizadores s√£o utilizados no processo de aprendizagem de redes neurais/deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT9z_L6HfCy7"
      },
      "source": [
        "*   **Gradiente Estoc√°stico (SGD)**\n",
        "\n",
        "Este √© um dos algoritmos de otimiza√ß√£o mais simples e populares. Ele atualiza os pesos da rede em cada passo de treinamento usando o gradiente da fun√ß√£o de erro calculada em **uma √∫nica amostra aleat√≥ria** dos dados de treinamento ou **um conjunto aleat√≥rio de amostras** (mini-batch). Ele usa uma taxa de aprendizagem (*learning rate*) para controlar o tamanho do passo de atualiza√ß√£o dos pesos. O SGD √© r√°pido e f√°cil de implementar, mas pode sofrer de instabilidade em algumas configura√ß√µes.\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:908/1*bKSddSmLDaYszWllvQ3Z6A.png\" width=\"550\" height=\"250\">\n",
        "</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/max/1400/1*tQTcGTLZqnI5rp3JYO_4NA.png\" width=\"550\" height=\"250\">\n",
        "</div>\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "*   **AdaGrad**\n",
        "\n",
        "AdaGrad, ou *Adaptive Gradient*, ajusta a taxa de aprendizado de cada peso individualmente, dependendo de sua hist√≥ria de atualiza√ß√µes anteriores. Isso significa que **pesos que foram atualizados frequentemente no passado ter√£o uma taxa de aprendizado menor, enquanto pesos que foram atualizados com menos frequ√™ncia ter√£o uma taxa de aprendizado maior**. A atualiza√ß√£o dos par√¢metros $Œò$ em cada etapa $t$ √© dada por[$^{[3]}$](https://medium.com/konvergen/an-introduction-to-adagrad-f130ae871827):\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RLCYjNDv9FhoshKA1S_P3A.png\">\n",
        "</div>\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*4M7mKsU1zDZzEAXGgIPoIQ.png\">\n",
        "</div>\n",
        "Simplificando:\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*fvFYO73z8lKKeliFPj4KlA.png\">\n",
        "</div>\n",
        "\n",
        "Sendo ùúº √© a taxa de aprendizado, $g(t)$ √© o gradiente da fun√ß√£o de perda no tempo $t$, $G(t)$ √© uma matriz diagonal contendo a soma dos quadrados dos gradientes anteriores para cada peso at√© o tempo $t$ e ùú∫ √© um valor pequeno adicionado para evitar divis√£o por zero.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "*   **RMSProp**\n",
        "\n",
        "RMSprop, ou *Root Mean Square propagation*, tamb√©m ajusta a taxa de aprendizado para cada peso individualmente, mas **usa uma m√©dia m√≥vel dos gradientes quadrados anteriores para controlar a taxa de aprendizado**, previnindo que a taxa se torne muito pequena e o tempo de converg√™ncia muito alto. Isso ajuda a ajustar a taxa de aprendizado de forma adaptativa para cada peso, com base na frequ√™ncia e na magnitude das atualiza√ß√µes anteriores. A atualiza√ß√£o do par√¢metro $w$ em cada etapa $t$ √© dada por[$^{[4]}$](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a):\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*R5q73YgObhvSPws9RpNmPA.png\" width=\"550\" height=\"250\">\n",
        "</div>\n",
        "\n",
        "Sendo:\n",
        "</br>\n",
        "$E[g^{2}]$ √© a m√©dia m√≥vel dos gradientes ao quadrado.</br>\n",
        "$\\frac{‚àÇC}{‚àÇw}$ √© o gradiente da fun√ß√£o de custo em rela√ß√£o ao peso.</br>\n",
        "ùù∂ √© a taxa de aprendizado.</br>\n",
        "**Œ≤** √© o par√¢metro da m√©dia m√≥vel (bom valor padr√£o - 0,9).\n",
        "\n",
        "Esta √© uma op√ß√£o mais est√°vel que o SGD e pode ser √∫til para conjuntos de dados com alta vari√¢ncia ou ru√≠do.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "*   **Adam**\n",
        "\n",
        "O Adam (*Adaptive Moment Estimation*) √© uma extens√£o do **RMSProp**, que calcula as **m√©dias m√≥veis exponenciais dos gradientes de primeiro e segundo momentos**. Isso ajuda a acelerar o processo de treinamento de uma rede neural, ajustando a taxa de aprendizado adaptativamente para cada peso.\n",
        "\n",
        "A atualiza√ß√£o do Adam para os pesos $w$ em cada etapa $t$ √© dada por:\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://ambrapaliaidata.blob.core.windows.net/ai-storage/articles/image_yQDDcFh.png\" width=\"550\" height=\"250\">\n",
        "</div>\n",
        "\n",
        "Onde:</br>\n",
        "$w_{t+1}$ √© o valor atualizado do peso W na etapa t+1.</br>\n",
        "$w_{t}$ √© o valor atual do peso W na etapa t.</br>\n",
        "ùù∂ √© a taxa de aprendizado.</br>\n",
        "$m_{t}$ √© a m√©dia m√≥vel exponencial dos gradientes de primeiro momento na etapa t.</br>\n",
        "$v_{t}$ √© a m√©dia m√≥vel exponencial dos gradientes de segundo momento na etapa t.</br>\n",
        "ùú∫ √© um valor pequeno adicionado para evitar a divis√£o por zero.</br>\n",
        "$Œ≤_{1}$ - par√¢metro da m√©dia m√≥vel do primeiro momento (bom valor padr√£o - 0,9).</br>\n",
        "$Œ≤_{2}$ - par√¢metro da m√©dia m√≥vel do segundo momento (bom valor padr√£o - 0,999).\n",
        "\n",
        "O Adam tamb√©m usa a t√©cnica de bias-correction ($\\hat{m}_{t}$, $\\hat{v}_{t}$), que corrige o vi√©s das m√©dias m√≥veis exponenciais durante as primeiras etapas de treinamento."
      ]
    }
  ]
}